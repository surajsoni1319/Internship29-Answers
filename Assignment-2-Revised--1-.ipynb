{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.1.5)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.26.10)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: cryptography>=1.3.4; extra == \"secure\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2.9.2)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14; extra == \"secure\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (19.1.0)\n",
      "Requirement already satisfied: idna>=2.0.0; extra == \"secure\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2.10)\n",
      "Requirement already satisfied: certifi; extra == \"secure\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (19.3.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cryptography>=1.3.4; extra == \"secure\"->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "Then click the search button.\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "for i in experience_tags[0:9]:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp) \n",
    "experience_required.insert(0,'26MAY-24JUNE')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Exp_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Data Analyst (Sr Exe / Asst Manager)</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>Talent Leads HR Solutions Pvt Ltd</td>\n",
       "      <td>26MAY-24JUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst / Product Analyst / Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>Benchire</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst - Data and Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Intel</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr.Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, karnataka\\n(WFH during Co...</td>\n",
       "      <td>Collabera</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hiring Data Analyst - Forensics, KPMG India</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Kolkata, Mumbai, G...</td>\n",
       "      <td>KPMG India</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Hiring For Data Analyst For Contract To...</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad/Secunderabad</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr. Analyst Advanced Analytics and Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Kennametal</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0      Business Data Analyst (Sr Exe / Asst Manager)   \n",
       "1  Data Analyst / Product Analyst / Business Analyst   \n",
       "2                  Data Analyst - Data and Analytics   \n",
       "3                                Senior Data Analyst   \n",
       "4                           Sr.Business Data Analyst   \n",
       "5                                     Data Analyst I   \n",
       "6        Hiring Data Analyst - Forensics, KPMG India   \n",
       "7  Urgent Hiring For Data Analyst For Contract To...   \n",
       "8                                     Data Analyst I   \n",
       "9    Sr. Analyst Advanced Analytics and Data Science   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0                   Bangalore/Bengaluru, Delhi / NCR   \n",
       "1                   Bangalore/Bengaluru, Delhi / NCR   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Bangalore/Bengaluru, karnataka\\n(WFH during Co...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Bangalore/Bengaluru, Noida, Kolkata, Mumbai, G...   \n",
       "7        Bangalore/Bengaluru, Hyderabad/Secunderabad   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                        Company_name  Exp_required  \n",
       "0  Talent Leads HR Solutions Pvt Ltd  26MAY-24JUNE  \n",
       "1                           Benchire       3-6 Yrs  \n",
       "2                              Intel       2-5 Yrs  \n",
       "3                           Flipkart       3-6 Yrs  \n",
       "4                          Collabera       3-7 Yrs  \n",
       "5                             Cerner      6-11 Yrs  \n",
       "6                         KPMG India       1-4 Yrs  \n",
       "7                          TeamLease       2-6 Yrs  \n",
       "8                             Cerner       5-8 Yrs  \n",
       "9                         Kennametal      5-10 Yrs  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name,'Exp_required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "Then click the search button.\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data.# 2)Naukri Data-Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring For Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science - Senior Software Engineer</td>\n",
       "      <td>Bangalore/Bengaluru, Noida</td>\n",
       "      <td>Paytm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dataiku Consultant</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research and Development -AI/ML -(PhD )</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Science Engineer</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Science - Engineering Manager</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Mumbai</td>\n",
       "      <td>Paytm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "1                   Hiring For Senior Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5            Data Science - Senior Software Engineer   \n",
       "6                                 Dataiku Consultant   \n",
       "7            Research and Development -AI/ML -(PhD )   \n",
       "8                       Senior Data Science Engineer   \n",
       "9                 Data Science - Engineering Manager   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "1                          Bangalore/Bengaluru, Pune   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                         Bangalore/Bengaluru, Noida   \n",
       "6                 Bangalore/Bengaluru, Pune, Chennai   \n",
       "7  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "8  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...   \n",
       "9                 Bangalore/Bengaluru, Noida, Mumbai   \n",
       "\n",
       "                      Company_name  \n",
       "0                            Wipro  \n",
       "1  TATA CONSULTANCY SERVICES (TCS)  \n",
       "2                Applied Materials  \n",
       "3                Applied Materials  \n",
       "4                Applied Materials  \n",
       "5                            Paytm  \n",
       "6                            Wipro  \n",
       "7                              EXL  \n",
       "8                Fractal Analytics  \n",
       "9                            Paytm  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "location_delhi=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "location_delhi.click()\n",
    "\n",
    "\n",
    "salary_filter=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "exp_Reqd=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    title=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "    for i in title[0:10]:\n",
    "        job_title.append(i.text)\n",
    "    location=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "    for i in location[0:10]:\n",
    "        job_location.append(i.text)    \n",
    "    company=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for i in company[0:10]:\n",
    "        company_name.append(i.text)\n",
    "    experience=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "    for i in experience[0:10]:\n",
    "        exp_Reqd.append(i.text)       \n",
    "    next_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(exp_Reqd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>EXL</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Associate - Data Science</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...</td>\n",
       "      <td>Black Turtle</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Science Associate</td>\n",
       "      <td>Delhi / NCR(Vaishali)</td>\n",
       "      <td>Kreate Energy</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Job_title  \\\n",
       "0            Data Scientist - Noida/Bangalore   \n",
       "1             DigitalBCG GAMMA Data Scientist   \n",
       "2             Senior Associate - Data Science   \n",
       "3  Data Scientist For Healthcare Product team   \n",
       "4  Data Scientist For Healthcare Product team   \n",
       "5              Data Scientist - MIND Infotech   \n",
       "6           Data Scientist - Engine Algorithm   \n",
       "7                      Data Science Associate   \n",
       "8                              Data Scientist   \n",
       "9                              Data Scientist   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0                         Noida, Bangalore/Bengaluru   \n",
       "1                     New Delhi, Bangalore/Bengaluru   \n",
       "2  Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...   \n",
       "3          Delhi / NCR, Chennai, Bangalore/Bengaluru   \n",
       "4          Delhi / NCR, Chennai, Bangalore/Bengaluru   \n",
       "5                                              Noida   \n",
       "6  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "7                              Delhi / NCR(Vaishali)   \n",
       "8             Delhi / NCR, Pune, Bangalore/Bengaluru   \n",
       "9             Delhi / NCR, Pune, Bangalore/Bengaluru   \n",
       "\n",
       "                               Company_name Experience  \n",
       "0                                       EXL   5-10 Yrs  \n",
       "1                   Boston Consulting Group    2-5 Yrs  \n",
       "2                              Black Turtle    4-7 Yrs  \n",
       "3                  SECUREKLOUD TECHNOLOGIES    2-7 Yrs  \n",
       "4                  SECUREKLOUD TECHNOLOGIES    2-7 Yrs  \n",
       "5  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED    4-8 Yrs  \n",
       "6                              Primo Hiring    1-3 Yrs  \n",
       "7                             Kreate Energy    2-4 Yrs  \n",
       "8   Mount Talent Consulting Private Limited    2-4 Yrs  \n",
       "9   Mount Talent Consulting Private Limited    2-4 Yrs  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Job_title':job_title,'Job_location':job_location,'Company_name':company_name,'Experience':exp_Reqd})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "Brand\n",
    "Product Description\n",
    "Price\n",
    "To scrape the data you have to go through following steps:\n",
    "Go to flipkart webpage by url https://www.flipkart.com/\n",
    "Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunglasses=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sunglasses.send_keys('sunglasses')\n",
    "\n",
    "pop_up=driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "pop_up.click()\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "prod_desciption=[]\n",
    "price=[]\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    for i in brands[0:100]:\n",
    "        brand.append(i.text)\n",
    "    Product_desc=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in Product_desc[0:100]:\n",
    "        prod_desciption.append(i.text)    \n",
    "    prices=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in prices[0:100]:\n",
    "        price.append(i.text)\n",
    "    next_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120\n"
     ]
    }
   ],
   "source": [
    "print(len(brand),len(prod_desciption),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Prod_desc</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (53)</td>\n",
       "      <td>₹313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAHAAZIL</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Wa...</td>\n",
       "      <td>₹189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>₹268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (50)</td>\n",
       "      <td>₹569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Lee Topper</td>\n",
       "      <td>Riding Glasses Wrap-around Sunglasses (Free Size)</td>\n",
       "      <td>₹299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Mi</td>\n",
       "      <td>Polarized Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized Retro Square Sunglasses (61)</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                                          Prod_desc Price\n",
       "0      Silver Kartz           UV Protection Clubmaster Sunglasses (53)  ₹313\n",
       "1         ROYAL SON   Polarized, UV Protection Aviator Sunglasses (57)  ₹759\n",
       "2          DAHAAZIL  UV Protection, Night Vision, Riding Glasses Wa...  ₹189\n",
       "3          Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹639\n",
       "4            SUNBEE  UV Protection, Polarized Wayfarer Sunglasses (...  ₹268\n",
       "..              ...                                                ...   ...\n",
       "115       ROYAL SON          UV Protection Rectangular Sunglasses (50)  ₹569\n",
       "116  kingsunglasses                UV Protection Round Sunglasses (54)  ₹214\n",
       "117      Lee Topper  Riding Glasses Wrap-around Sunglasses (Free Size)  ₹299\n",
       "118              Mi           Polarized Aviator Sunglasses (Free Size)  ₹949\n",
       "119       ROYAL SON             Polarized Retro Square Sunglasses (61)  ₹699\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Brand':brand,'Prod_desc':prod_desciption,'Price':price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to \n",
    "go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "you have to scrape the below mention attributes.\n",
    "These are \n",
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review\n",
    "- You have to scrape this data for first 100 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "short_review=[]\n",
    "complete_review=[]\n",
    "stars=[]\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping 10 pages url\n",
    "url_1 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "for i in url_1:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "url_2 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M']\")\n",
    "for i in url_2:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    #for scrapping the number of stars\n",
    "    for j in driver.find_elements(By.XPATH,\"//div[@class='col _2wzgFH K0kLPL']/div[1]/div[1]\"):\n",
    "        stars.append(j.text)\n",
    "    #for scrapping the short review\n",
    "    for k in driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\"):\n",
    "        short_review.append(k.text)\n",
    "    #for scrapping the complete review\n",
    "    for l in driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']/div/div\"):\n",
    "        complete_review.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(stars),len(short_review),len(complete_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_summary</th>\n",
       "      <th>Full_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.\\nAnd I truly don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Nice value for money good and best price I pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Don’t expect much from front camera… especiall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Absolutely powerful gadget. Loved it’s look! S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating         Review_summary  \\\n",
       "0       5         Simply awesome   \n",
       "1       5       Perfect product!   \n",
       "2       5    Best in the market!   \n",
       "3       5     Highly recommended   \n",
       "4       5      Worth every penny   \n",
       "..    ...                    ...   \n",
       "95      5                 Super!   \n",
       "96      5              Wonderful   \n",
       "97      5    Best in the market!   \n",
       "98      5  Mind-blowing purchase   \n",
       "99      5      Worth every penny   \n",
       "\n",
       "                                         Full_summary  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   What a camera .....just awesome ..you can feel...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  This is my first ever iPhone.\\nAnd I truly don...  \n",
       "96  Nice value for money good and best price I pho...  \n",
       "97  Don’t expect much from front camera… especiall...  \n",
       "98  Absolutely powerful gadget. Loved it’s look! S...  \n",
       "99  Undoubtedly Iphone 11 is the most successful m...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Rating':stars,'Review_summary':short_review,'Full_summary':complete_review})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6:Scrape data for first 100 sneakers you find when you visit flipkart.comand search for “sneakers” in the search field.You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "-  Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sneakers=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sneakers.send_keys('sneakers')\n",
    "\n",
    "pop_up=driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "pop_up.click()\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements(By.CLASS_NAME,'_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    prices=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    desc=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\" or @class=\"IRpwTa _2-ICcC\"]')#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "        \n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 160 160\n"
     ]
    }
   ],
   "source": [
    "print(len(brand),len(description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Prod_desc</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>₹219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolute comfort</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>x KL Denver Sneakers For Men</td>\n",
       "      <td>₹1,679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Roadster</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers White Shoes For Girls And Wome...</td>\n",
       "      <td>₹549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>X-Ray Sneakers For Men</td>\n",
       "      <td>₹2,602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Detok</td>\n",
       "      <td>Light Weight Gym Shoe |Tracking Shoe| Running ...</td>\n",
       "      <td>₹331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Bretton</td>\n",
       "      <td>Classy Stylish Ayasa Sneakers Sneakers For Men</td>\n",
       "      <td>₹251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                          Prod_desc  \\\n",
       "0              BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men   \n",
       "1            URBANBOX                          Sneakers Sneakers For Men   \n",
       "2            KWIK FIT  Kwik FIT casual sneaker shoes and partywear sh...   \n",
       "3    Absolute comfort                                   Sneakers For Men   \n",
       "4                PUMA                       x KL Denver Sneakers For Men   \n",
       "..                ...                                                ...   \n",
       "155          Roadster                                   Sneakers For Men   \n",
       "156      Robbie jones  Casual Sneakers White Shoes For Girls And Wome...   \n",
       "157              PUMA                             X-Ray Sneakers For Men   \n",
       "158             Detok  Light Weight Gym Shoe |Tracking Shoe| Running ...   \n",
       "159           Bretton     Classy Stylish Ayasa Sneakers Sneakers For Men   \n",
       "\n",
       "      Price  \n",
       "0      ₹284  \n",
       "1      ₹219  \n",
       "2      ₹449  \n",
       "3      ₹298  \n",
       "4    ₹1,679  \n",
       "..      ...  \n",
       "155  ₹1,087  \n",
       "156    ₹549  \n",
       "157  ₹2,602  \n",
       "158    ₹331  \n",
       "159    ₹251  \n",
       "\n",
       "[160 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Brand':brand,'Prod_desc':description,'Price':price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8:  Go to webpage https://www.amazon.in/Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7”\n",
    "After setting the filters scrape first 10 laptops data.You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price# 8)Amazon Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop=driver.find_element(By.XPATH,\"//div[@class='nav-search-field ']//input\")\n",
    "laptop.send_keys('Laptop')\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# code to filter i7 CPu-\n",
    "cpu_filter1=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[4]/li[12]/span/a/div\")\n",
    "cpu_filter1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "price=[]\n",
    "Ratings=[]\n",
    "Rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    title=driver.find_elements(By.XPATH,\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "    for i in title[0:10]:\n",
    "        Title.append(i.text) \n",
    "    prices=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "    for i in prices[0:10]:\n",
    "        price.append(i.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_rat=driver.find_elements(By.XPATH,\"//div[@class='a-row a-size-small']/span\")\n",
    "for i in titles_rat:\n",
    "    Rating.append(i.get_attribute(\"aria-label\"))\n",
    "rating=[]\n",
    "for i in range(0,len(Rating))[0:20]:\n",
    "    if i == 0 or  i/2 == i//2:\n",
    "        rating.append(Rating[i][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.3', '4.5', '4.2', '4.4', '3.8', '3.6', '4.0', '4.2', '3.8', '4.3']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Title),len(price),len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LG Gram 16 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>80,990</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG Gram 17 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>91,990</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>57,510</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>89,990</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Pro Intel Core i7 11th G...</td>\n",
       "      <td>75,990</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>64,592</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...</td>\n",
       "      <td>86,990</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>82,990</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Renewed) Dell Latitude E7470 Intel Core i7 6t...</td>\n",
       "      <td>82,990</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LG Gram 16 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>39,177</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Price Ratings\n",
       "0  LG Gram 16 Intel Evo 11th Gen i7 Thin & Light ...  80,990     4.3\n",
       "1  LG Gram 17 Intel Evo 11th Gen i7 Thin & Light ...  91,990     4.5\n",
       "2  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...  57,510     4.2\n",
       "3  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  89,990     4.4\n",
       "4  Lenovo IdeaPad Slim 5 Pro Intel Core i7 11th G...  75,990     3.8\n",
       "5  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...  64,592     3.6\n",
       "6  Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...  86,990     4.0\n",
       "7  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  82,990     4.2\n",
       "8  (Renewed) Dell Latitude E7470 Intel Core i7 6t...  82,990     3.8\n",
       "9  LG Gram 16 Intel Evo 11th Gen i7 Thin & Light ...  39,177     4.3"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Title':Title,'Price':price,'Ratings':rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida \n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_button=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[5]/a\")\n",
    "job_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "designation=driver.find_element(By.XPATH,\"//input[@class='input tt-input']\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"//button[@class='ab_btn search-btn round']\")\n",
    "search.click()\n",
    "\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i\")\n",
    "location.click()\n",
    "\n",
    "noida_loc=driver.find_element(By.XPATH,\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[5]/div/label\")\n",
    "noida_loc.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "company=[]\n",
    "days_ago=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    title=driver.find_elements(By.XPATH,\"//a[@class='title noclick']\")\n",
    "    for i in title[0:10]:\n",
    "        job_title.append(i.text)\n",
    "    Compny=driver.find_elements(By.XPATH,\"//p[@class='company body-medium']\")\n",
    "    for i in Compny[0:10]:\n",
    "        company.append(i.text)    \n",
    "\n",
    "     \n",
    "    next_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_days=driver.find_elements(By.XPATH,\"//div[@class='other-info']/span\")\n",
    "for i in no_of_days[:]:\n",
    "     days_ago.append(i.text.split()[0])\n",
    "#Extracting every 3rd item from the list starting from index 0\n",
    "days_posted=days_ago[0::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(company),len(days_posted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job_Posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>15d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Intern</td>\n",
       "      <td>CARE HEALTH INSURANCE LIMITED</td>\n",
       "      <td>2d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist- Forecasting and R or Python</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>22d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LatentBridge - Data Scientist - Python/R (2-6 ...</td>\n",
       "      <td>Latent bridge</td>\n",
       "      <td>6d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Machine Learning Algorithms (...</td>\n",
       "      <td>Dew Solutions Pvt. Ltd.</td>\n",
       "      <td>13d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Machine Learning/Big Data (0-...</td>\n",
       "      <td>InfoEdge India Ltd.</td>\n",
       "      <td>20d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Info Edge - Data Scientist - Machine Learning/...</td>\n",
       "      <td>Info Edge India Limited</td>\n",
       "      <td>21d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Info Edge - Senior Data Scientist - Machine Le...</td>\n",
       "      <td>Info Edge India Limited</td>\n",
       "      <td>21d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - NLP</td>\n",
       "      <td>CRMnext</td>\n",
       "      <td>16d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Science Trainer | AI | Machine Learning |...</td>\n",
       "      <td>Careerera</td>\n",
       "      <td>8d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                              Data Scientist Intern   \n",
       "2        Data Scientist- Forecasting and R or Python   \n",
       "3  LatentBridge - Data Scientist - Python/R (2-6 ...   \n",
       "4  Data Scientist - Machine Learning Algorithms (...   \n",
       "5  Data Scientist - Machine Learning/Big Data (0-...   \n",
       "6  Info Edge - Data Scientist - Machine Learning/...   \n",
       "7  Info Edge - Senior Data Scientist - Machine Le...   \n",
       "8                               Data Scientist - NLP   \n",
       "9  Data Science Trainer | AI | Machine Learning |...   \n",
       "\n",
       "                                          Company Job_Posted  \n",
       "0  Optum Global Solutions (India) Private Limited        15d  \n",
       "1                   CARE HEALTH INSURANCE LIMITED         2d  \n",
       "2                   GENPACT India Private Limited        22d  \n",
       "3                                   Latent bridge         6d  \n",
       "4                         Dew Solutions Pvt. Ltd.        13d  \n",
       "5                             InfoEdge India Ltd.        20d  \n",
       "6                         Info Edge India Limited        21d  \n",
       "7                         Info Edge India Limited        21d  \n",
       "8                                         CRMnext        16d  \n",
       "9                                       Careerera         8d  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Title':job_title,'Company':company,'Job_Posted':days_posted})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Desktop\\Fliprobo notes\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.ambitionbox.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_button=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[3]\")\n",
    "salary_button.click()\n",
    "\n",
    "salary_tag=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[3]/div/ul/li[1]/div/div[2]/p\")\n",
    "salary_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=driver.find_element(By.XPATH,'/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p')\n",
    "tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "company=[]\n",
    "total_salary = []\n",
    "avg_Sal=[]\n",
    "min_Sal=[]\n",
    "max_Sal=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    company_tags =driver.find_elements(By.CLASS_NAME,'company-info')[:10]\n",
    "    for i in company_tags:\n",
    "         company.append(i.text.split('\\n')[0])     \n",
    "    total_tags = driver.find_elements(By.CLASS_NAME,'datapoints')\n",
    "    for x in total_tags:\n",
    "         total_salary.append(x.text.split(' ')[2])    \n",
    "    avg_salary=driver.find_elements(By.XPATH,'//div[@class=\"average-indicator-wrapper\"]')\n",
    "    for i in avg_salary[0:10]:\n",
    "        avg_Sal.append(i.text)\n",
    "    minimum_sal=driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]')\n",
    "    for i in minimum_sal[0:10]:\n",
    "        min_Sal.append(i.text)  \n",
    "    maximum_Sal=driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]')\n",
    "    for i in maximum_Sal[0:10]:\n",
    "        max_Sal.append(i.text)  \n",
    "    exp_tags = driver.find_elements(By.CLASS_NAME,'sbold-list-header')[:10]\n",
    "    for y in exp_tags:\n",
    "        experience.append(y.text.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(company),len(total_salary),len(avg_Sal),len(min_Sal),len(max_Sal),len(experience))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Total_Sal_Record</th>\n",
       "      <th>Avg_Sal</th>\n",
       "      <th>Min_Sal</th>\n",
       "      <th>Max_Sal</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>12</td>\n",
       "      <td>₹ 30.6L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>33</td>\n",
       "      <td>₹ 20.8L</td>\n",
       "      <td>₹ 36.0L</td>\n",
       "      <td>₹ 36.0L</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZS</td>\n",
       "      <td>15</td>\n",
       "      <td>₹ 16.7L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optum</td>\n",
       "      <td>32</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliance Jio</td>\n",
       "      <td>21</td>\n",
       "      <td>₹ 15.7L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>89</td>\n",
       "      <td>₹ 15.5L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>2-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>51</td>\n",
       "      <td>₹ 14.8L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>2-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>57</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 22.5L</td>\n",
       "      <td>₹ 22.5L</td>\n",
       "      <td>2-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>21</td>\n",
       "      <td>₹ 13.2L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Deloitte</td>\n",
       "      <td>69</td>\n",
       "      <td>₹ 12.8L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>2-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Company Total_Sal_Record  Avg_Sal  Min_Sal  Max_Sal Experience\n",
       "0            Walmart               12  ₹ 30.6L  ₹ 25.0L  ₹ 25.0L          3\n",
       "1           Ab Inbev               33  ₹ 20.8L  ₹ 36.0L  ₹ 36.0L        3-4\n",
       "2                 ZS               15  ₹ 16.7L  ₹ 15.0L  ₹ 15.0L          2\n",
       "3              Optum               32  ₹ 15.9L  ₹ 26.2L  ₹ 26.2L        3-4\n",
       "4       Reliance Jio               21  ₹ 15.7L  ₹ 11.0L  ₹ 11.0L        3-4\n",
       "5  Fractal Analytics               89  ₹ 15.5L  ₹ 22.0L  ₹ 22.0L        2-4\n",
       "6    Tiger Analytics               51  ₹ 14.8L  ₹ 11.0L  ₹ 11.0L        2-4\n",
       "7       UnitedHealth               57  ₹ 14.0L  ₹ 22.5L  ₹ 22.5L        2-4\n",
       "8        EXL Service               21  ₹ 13.2L   ₹ 5.6L   ₹ 5.6L        3-4\n",
       "9           Deloitte               69  ₹ 12.8L  ₹ 26.2L  ₹ 26.2L        2-4"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Company':company,'Total_Sal_Record':total_salary,'Avg_Sal':avg_Sal,'Min_Sal':min_Sal,'Max_Sal':max_Sal,'Experience':experience})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
